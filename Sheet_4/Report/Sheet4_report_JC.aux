\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Energy (in log scale) vs iterations of backpropagation algorithm, for $N_G = 5$ Gaussian nodes. Training data in blue, validation data in red. For clarity, only three runs out of 20 are shown, each with only one point every 100 iterations. Parameters in the competitive learning phase: $10^5$ iterations, learning rate $\eta _c = 0.02$, neighbourhood function width $\sigma = 0.1$. Parameters in the backpropagation phase: $10^4$\nobreakspace  {}iterations, learning rate $\eta _b = 0.1$, activation function $g(b) = \qopname  \relax o{tanh}(\beta b)$, with $\beta = 0.5$.\relax }}{1}{figure.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{1a_energy}{{1}{1}{Energy (in log scale) vs iterations of backpropagation algorithm, for $N_G = 5$ Gaussian nodes. Training data in blue, validation data in red. For clarity, only three runs out of 20 are shown, each with only one point every 100 iterations. Parameters in the competitive learning phase: $10^5$ iterations, learning rate $\eta _c = 0.02$, neighbourhood function width $\sigma = 0.1$. Parameters in the backpropagation phase: $10^4$~iterations, learning rate $\eta _b = 0.1$, activation function $g(b) = \tanh (\beta b)$, with $\beta = 0.5$.\relax }{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Classification error vs iterations of backpropagation algorithm, for $N_G = 5$ Gaussian nodes. Training data in blue, validation data in red. For clarity, only three runs out of 20 are shown, each with only one point every 100 iterations. Parameters in the competitive learning phase: $10^5$ iterations, learning rate $\eta _c = 0.02$, neighbourhood function width $\sigma = 0.1$. Parameters in the backpropagation phase: $10^4$\nobreakspace  {}iterations, learning rate $\eta _b = 0.1$, activation function $g(b) = \qopname  \relax o{tanh}(\beta b)$, with $\beta = 0.5$.\relax }}{2}{figure.caption.4}}
\newlabel{1b_error}{{2}{2}{Classification error vs iterations of backpropagation algorithm, for $N_G = 5$ Gaussian nodes. Training data in blue, validation data in red. For clarity, only three runs out of 20 are shown, each with only one point every 100 iterations. Parameters in the competitive learning phase: $10^5$ iterations, learning rate $\eta _c = 0.02$, neighbourhood function width $\sigma = 0.1$. Parameters in the backpropagation phase: $10^4$~iterations, learning rate $\eta _b = 0.1$, activation function $g(b) = \tanh (\beta b)$, with $\beta = 0.5$.\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Left panel: data points, plotted in different colours and with different symbols according to the sign of the corresponding class, weight vectors as red crosses, and decision boundary as red line. Right panel: pseudo-colour map representation of the network output for a grid of equally spaced points covering the same portion of the input space. Same parameters as in \textbf  {1a} and \textbf  {1b}, 5 Gaussian nodes.\relax }}{3}{figure.caption.6}}
\newlabel{fig:1c_boundary}{{3}{3}{Left panel: data points, plotted in different colours and with different symbols according to the sign of the corresponding class, weight vectors as red crosses, and decision boundary as red line. Right panel: pseudo-colour map representation of the network output for a grid of equally spaced points covering the same portion of the input space. Same parameters as in \textbf {1a} and \textbf {1b}, 5 Gaussian nodes.\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Classification error vs iterations of backpropagation algorithm, for $N_G = 20$ Gaussian nodes. Training data in blue, validation data in red. For clarity, only three runs out of 20 are shown, each with only one point every 100 iterations. Parameters in the competitive learning phase: $10^5$ iterations, learning rate $\eta _c = 0.02$, neighbourhood function width $\sigma = 0.1$. Parameters in the backpropagation phase: $10^4$\nobreakspace  {}iterations, learning rate $\eta _b = 0.1$, activation function $g(b) = \qopname  \relax o{tanh}(\beta b)$, with $\beta = 0.5$.\relax }}{4}{figure.caption.8}}
\newlabel{2a_error}{{4}{4}{Classification error vs iterations of backpropagation algorithm, for $N_G = 20$ Gaussian nodes. Training data in blue, validation data in red. For clarity, only three runs out of 20 are shown, each with only one point every 100 iterations. Parameters in the competitive learning phase: $10^5$ iterations, learning rate $\eta _c = 0.02$, neighbourhood function width $\sigma = 0.1$. Parameters in the backpropagation phase: $10^4$~iterations, learning rate $\eta _b = 0.1$, activation function $g(b) = \tanh (\beta b)$, with $\beta = 0.5$.\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Left panel: data points, plotted in different colours and with different symbols according to the sign of the corresponding class, weight vectors as red crosses, and decision boundary as red line. Right panel: pseudo-colour map representation of the network output for a grid of equally spaced points covering the same portion of the input space. Same parameters as in \textbf  {2a}, 20 Gaussian nodes.\relax }}{5}{figure.caption.10}}
\newlabel{fig:2b_boundary}{{5}{5}{Left panel: data points, plotted in different colours and with different symbols according to the sign of the corresponding class, weight vectors as red crosses, and decision boundary as red line. Right panel: pseudo-colour map representation of the network output for a grid of equally spaced points covering the same portion of the input space. Same parameters as in \textbf {2a}, 20 Gaussian nodes.\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Classification error vs number of Gaussian nodes used for the competitive learning part. Training data in blue, validation data in red. Parameters in the competitive learning phase: $10^5$ iterations, learning rate $\eta _c = 0.02$, neighbourhood function width $\sigma = 0.1$. Parameters in the backpropagation phase: $5\cdot 10^3$\nobreakspace  {}iterations, learning rate $\eta _b = 0.1$, activation function $g(b) = \qopname  \relax o{tanh}(\beta b)$, with $\beta = 0.5$.\relax }}{6}{figure.caption.12}}
\newlabel{3_errorVsNodes}{{6}{6}{Classification error vs number of Gaussian nodes used for the competitive learning part. Training data in blue, validation data in red. Parameters in the competitive learning phase: $10^5$ iterations, learning rate $\eta _c = 0.02$, neighbourhood function width $\sigma = 0.1$. Parameters in the backpropagation phase: $5\cdot 10^3$~iterations, learning rate $\eta _b = 0.1$, activation function $g(b) = \tanh (\beta b)$, with $\beta = 0.5$.\relax }{figure.caption.12}{}}
